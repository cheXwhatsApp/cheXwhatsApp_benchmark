{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TorchVision repo\n",
    "!rm -r vision\n",
    "!git init\n",
    "!pip install pycocotools --quiet\n",
    "!pip install torchvision --quiet\n",
    "!git config --global --unset http.proxy \n",
    "!git config --global --unset https.proxy\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "!cp vision/references/detection/utils.py ./\n",
    "!cp vision/references/detection/transforms.py ./\n",
    "!cp vision/references/detection/coco_eval.py ./\n",
    "!cp vision/references/detection/engine.py ./\n",
    "!cp vision/references/detection/coco_utils.py ./\n",
    "\n",
    "!rm -r pngImages\n",
    "!mkdir pngImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pydicom\n",
    "from skimage.transform import resize\n",
    "\n",
    "## Torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "## Image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "## Helper libraries\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access and cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parent path to data\n",
    "path = '../path/to/Data/'\n",
    "\n",
    "boxCSV = 'stage_2_train_labels.csv'\n",
    "dataCSV = 'stage_2_detailed_class_info.csv'\n",
    "\n",
    "imageFolders = ['stage_2_test_images', 'stage_2_train_images']\n",
    "\n",
    "## First class is background\n",
    "classes = [_, 1]\n",
    "\n",
    "## Image data\n",
    "dataDF = pd.read_csv(path + dataCSV)\n",
    "## Box data\n",
    "boxDF = pd.read_csv(path + boxCSV)\n",
    "\n",
    "\n",
    "\n",
    "dataGlob = glob('../path/to/Data/*images/*.png')\n",
    "\n",
    "\n",
    "## Dictionary of image to path\n",
    "imagePaths = {os.path.basename(x)[:-4]: x for x in dataGlob}\n",
    "## Full image path data table\n",
    "dataPath = dataDF['patientId'].map(imagePaths.get)\n",
    "\n",
    "## Full path to images with bounding boxes\n",
    "boxPaths= boxDF['patientId'].map(imagePaths.get)\n",
    "## Isolate images with bounding boxes\n",
    "boxImages = pd.merge(left = boxDF, right = dataDF, left_on = 'patientId', right_on = 'patientId', how = 'inner')\n",
    "boxImages.dropna(axis = 0, inplace = True)\n",
    "boxDF.dropna(axis = 0, inplace = True)\n",
    "\n",
    "## Bounding box and label data groupings by image. Using boxDF due to inner join duplication\n",
    "xBox = boxDF.groupby('patientId')['x'].apply(np.array).reset_index()['x'].values\n",
    "yBox = boxDF.groupby('patientId')['y'].apply(np.array).reset_index()['y'].values\n",
    "wBox = boxDF.groupby('patientId')['width'].apply(np.array).reset_index()['width'].values\n",
    "hBox = boxDF.groupby('patientId')['height'].apply(np.array).reset_index()['height'].values\n",
    "## Group the finding labels together for the varying bounding boxes in each image\n",
    "boxLabel = boxDF.groupby('patientId')['Target'].apply(np.array).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "boxLabel['paths'] = \"../path/to/Data/stage_2_train_images/\" + boxLabel['patientId'] + \".png\"\n",
    "\n",
    "print(\"Number of images: \", len(boxLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert dicom files to jpg images and write to directory\n",
    "import imageio\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image,ExifTags\n",
    "\n",
    "def create_png_df():\n",
    "    imageDict = {'Directory':[], 'ID':[]}\n",
    "    \n",
    "    for i in range(len(boxLabel)):\n",
    "        try:\n",
    "            imageName = boxLabel.iloc[i]['patientId']\n",
    "            jpgName = imageName + ('.png')            \n",
    "            imageDict['Directory'].append(os.path.join('../path/to/Data/stage_2_train_images_png', jpgName))\n",
    "            imageDict['ID'].append(imageName)\n",
    "        except Exception as e:\n",
    "            print(i,e)\n",
    "\n",
    "    return pd.DataFrame(imageDict)\n",
    "\n",
    "pngImages = create_png_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pngImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dictionary object for each image with its bounding box coords and labels\n",
    "class LungData(torch.utils.data.Dataset):\n",
    "    def __init__(self, height, width, transforms = None):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.len = len(boxLabel)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    ## Overwrite and return the image dictionary\n",
    "    def __getitem__(self, index):\n",
    "        imagePath = pngImages['Directory'].iloc[index]\n",
    "\n",
    "        # Read and resize the image\n",
    "        origImage = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(origImage, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "        image = cv2.resize(image, (self.width, self.height), cv2.INTER_AREA)\n",
    "        image = image / 255.0\n",
    "        \n",
    "        \n",
    "        ## Combine all boxes for an image together\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        ## Original image shape for normalizing (both 1024)\n",
    "        Wimage = origImage.shape[1]\n",
    "        Himage = origImage.shape[0]\n",
    "\n",
    "        ## Create dictionary with image info and boxes\n",
    "        for member in range(len(boxLabel['Target'].iloc[index])):\n",
    "            labels.append(classes.index(boxLabel['Target'].iloc[index][member]))\n",
    "                          \n",
    "            xMin = xBox[index][member]\n",
    "            xMax = xBox[index][member] + wBox[index][member]\n",
    "            \n",
    "            yMin = yBox[index][member]\n",
    "            yMax = yBox[index][member] + hBox[index][member]\n",
    "            \n",
    "            xMinCorr = (xMin/Wimage) * self.width\n",
    "            xMaxCorr = (xMax/Wimage) * self.width\n",
    "            yMinCorr = (yMin/Himage) * self.height\n",
    "            yMaxCorr = (yMax/Himage) * self.height\n",
    "            \n",
    "            boxes.append([xMinCorr, yMinCorr, xMaxCorr, yMaxCorr])\n",
    "        \n",
    "        ## Convert bounding boxes to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        ## Calculate area of boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        ## Suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        ## Create output dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        imageId = torch.tensor([index])\n",
    "        # target[\"image_id\"] = imageId\n",
    "        target[\"image_id\"] = index\n",
    "        \n",
    "        target[\"image_path\"] = imagePath\n",
    "        \n",
    "        # Apply data transformations to reduce overfitting\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            \n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "               \n",
    "        return image, target\n",
    "    \n",
    "    ## Overwrite function to return length of dataset\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "## Check dataset length\n",
    "dataset = LungData(256, 256)\n",
    "print(\"Length of dataset: \", len(dataset), \"\\n\")\n",
    "\n",
    "## Sample index to show image dictionary output\n",
    "image, target = dataset[878]\n",
    "print(\"Image shape: \", image.shape, \"\\n\")\n",
    "print(\"Image dictionary object: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize an X-Ray image with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot an image and overlay its bounding box on top of it\n",
    "def plotImage(image, target, color):\n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(10,10)\n",
    "    a.imshow(image, cmap='gray')\n",
    "    ## get the context for drawing boxes\n",
    "    ax = plt.gca()\n",
    "    ## Plot each box\n",
    "    for i in range(len(target['boxes'])):\n",
    "        box = target['boxes'][i]\n",
    "        x, y, width, height  = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
    "        ## Create rectangle patch for bounding box\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth = 2, edgecolor = color, facecolor = 'none')\n",
    "        ax.add_patch(rect)\n",
    "        ## Draw text on top of box\n",
    "        label = \"%s\" % (classes[target['labels'][i]])\n",
    "        plt.text(x, y, label, color='red')\n",
    "    ## show the plot\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "## Plot a sample image\n",
    "image, target = dataset[200]\n",
    "plotImage(image, target, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ResNet-50-FPN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the base model to be trained\n",
    "def detectionModel(numClasses):\n",
    "    ## Load a model pretrained resnet model to speed training time\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    ## Get number of input features for the classifier\n",
    "    inFeatures = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    ## Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(inFeatures, numClasses) \n",
    "\n",
    "    return model\n",
    "\n",
    "## Data transformations during training to reduce overfit\n",
    "def createTransform(train):\n",
    "    if train:\n",
    "        return A.Compose([A.HorizontalFlip(0.5), ToTensorV2(p=1.0)], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    else:\n",
    "        return A.Compose([ToTensorV2(p=1.0)], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize train and test objects\n",
    "dataTrain = LungData(256, 256, createTransform(False))\n",
    "dataTest = LungData(256, 256, createTransform(False))\n",
    "\n",
    "## Split the dataset into train and test sets\n",
    "torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataTrain)).tolist()\n",
    "indices = torch.arange(len(dataTrain)).tolist()\n",
    "dataSplit = 0.2\n",
    "testSize = int(len(dataTrain)*dataSplit)\n",
    "dataTrain = torch.utils.data.Subset(dataTrain, indices[:-testSize])\n",
    "dataTest = torch.utils.data.Subset(dataTest, indices[-testSize:])\n",
    "\n",
    "## Define training and validation data loaders\n",
    "dataTrainLoader = torch.utils.data.DataLoader(dataTrain, batch_size=10, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\n",
    "dataTestLoader = torch.utils.data.DataLoader(dataTest, batch_size=10, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine if we can use a GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "## Initialize model\n",
    "numClasses = 2\n",
    "model = detectionModel(numClasses)\n",
    "\n",
    "# model.double()\n",
    "model.to(device)\n",
    "\n",
    "## Construct an optimizer and learning rate\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0055, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=0.0005)\n",
    "lrScheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose number of training epochs\n",
    "numEpochs = 5\n",
    "\n",
    "## Tensorboard writer\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "## Train one epoch at a time for numEpochs\n",
    "for epoch in range(numEpochs):\n",
    "    train_one_epoch(model, optimizer, dataTrainLoader, device, epoch, print_freq=10)\n",
    "    ## Update the learning rate\n",
    "    lrScheduler.step()\n",
    "    ## Evaluate on the test dataset\n",
    "    evaluate(model, dataTestLoader, device=device)\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"../path/to/save/epoch_{epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement bounding box threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold to determine best bounding box based on IOU threshold\n",
    "def thresholdBB(prediction, thresh):\n",
    "    ## Indeces of best bounding boxes\n",
    "    bestBox = []\n",
    "    for box in range(len(prediction['boxes'])):\n",
    "        if prediction['scores'][box] >= thresh:\n",
    "            bestBox.append(box)\n",
    "    newPrediction = prediction\n",
    "    newPrediction['boxes'] = newPrediction['boxes'][bestBox]\n",
    "    newPrediction['scores'] = newPrediction['scores'][bestBox]\n",
    "    newPrediction['labels'] = newPrediction['labels'][bestBox]\n",
    "    return newPrediction\n",
    "\n",
    "## Convert torch tensor to PIL image\n",
    "def torchToPIL(image):\n",
    "    return transforms.ToPILImage()(image).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random image from test dataset\n",
    "image, target = dataTest[57]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([image.to(device)])[0]\n",
    "    \n",
    "print('Predicted number of boxes: ', len(prediction['labels']))\n",
    "print('True number of boxes: ', len(target['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predicted vs. GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot both the predicted and ground truth boxes on the image\n",
    "def plotPredictGT(image,width, height, targetGT, targetPredict, colorGT, colorPredict, isWP = False):\n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(10,10)\n",
    "    image = image.reshape(width, height, -1)\n",
    "    a.imshow(image, cmap='gray')\n",
    "    ## get the context for drawing boxes\n",
    "    ax = plt.gca()\n",
    "    ## Plot each box\n",
    "    for i in range(len(targetGT['boxes'])):\n",
    "        box = targetGT['boxes'][i]\n",
    "        x, y, width, height  = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
    "        ## Create rectangle patch for bounding box\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth = 2, edgecolor = colorGT, facecolor = 'none')\n",
    "        ax.add_patch(rect)\n",
    "        ## Draw text on top of box\n",
    "        label = \"%s\" % (classes[targetGT['labels'][i]])\n",
    "        plt.text(x, y, label, color= colorGT)\n",
    "    for i in range(len(targetPredict['boxes'].cpu())):\n",
    "        box = targetPredict['boxes'][i].cpu()\n",
    "        x, y, width, height  = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
    "        ## Create rectangle patch for bounding box\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth = 2, edgecolor = colorPredict, facecolor = 'none')\n",
    "        ax.add_patch(rect)\n",
    "        ## Draw text on top of box\n",
    "        label = \"%s   %0.2f\" % (classes[targetPredict['labels'][i].cpu()], targetPredict['scores'][i].cpu())\n",
    "        plt.text(x, y, label, color= colorPredict)\n",
    "        \n",
    "    \n",
    "    ## show the plot\n",
    "    plt.show()\n",
    "    \n",
    "## Plot a sample image. Blue is predicted, red is GT\n",
    "threshPredict = thresholdBB(prediction, 0.55)\n",
    "plotPredictGT(image.cpu(), 256, 256, target, threshPredict, 'r', 'b')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sahil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
